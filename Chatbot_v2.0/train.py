from transformers import AutoModel, AutoTokenizer, BertTokenizer
import torch
import sys
import pandas as pd
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import Dataset, DataLoader
from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertConfig, pipeline
from datasets import load_metric
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import numpy as np
import os
import json

from utils import *
from dataloader import *
from model import *

    
# !git clone https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
# !git clone https://github.com/doublems/korean-bad-words
# !git clone https://github.com/organization/Gentleman/

def main():
    
    #ë¹„ì†ì–´ ì‚¬ì „ ë§Œë“¤ê¸°
    # ë¹„ì†ì–´ ì‚¬ì „ 01
    slang_list_01 = []
    f = open("./List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/ko", 'r')
    lines = f.readlines()
    for line in lines:
        line = line.replace("\n", "")
        slang_list_01.append(line)
    f.close()

    # ë¹„ì†ì–´ ì‚¬ì „ 02
    slang_list_02 = []
    f = open("./korean-bad-words/korean-bad-words.md", 'r')
    lines = f.readlines()
    for line in lines:
        line = line.replace("\n", "")
        slang_list_02.append(line)
    f.close()

    # ë¹„ì†ì–´ ì‚¬ì „ 03
    with open('./Gentleman/resources/badwords.json') as json_file:
        json_data = json.load(json_file)
    slang_list_03 = json_data["badwords"]

    # ë¹„ì†ì–´ ì‚¬ì „ í†µí•©
    slang_list = slang_list_01 + slang_list_02 + slang_list_03
    slang_list = list(set(slang_list))
    
    #ë¹„ì†ì–´ ê³¨ë¼ë‚´ëŠ” í•¨ìˆ˜
    def find_Badword(sent) :
        Badin = False
        for word in slang_list :
            if word in sent :
                Badin = True
        return Badin
    
    #Data ë¶ˆëŸ¬ì˜¤ê¸°
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    seed_everything()
    
    dataset = pd.read_csv('ChatbotData.csv')
    
    dataset.drop_duplicates(subset=['Q'], inplace=True)
    #label ë³„ ë°ì´í„°
    dataset_zero=pd.DataFrame({"Question":dataset['Q'][:5261], "answer":dataset['A'][:5261]})
    dataset_one=pd.DataFrame({"Question":dataset['Q'][5261:8743], "answer":dataset['A'][5261:8743]})
    dataset_two=pd.DataFrame({"Question":dataset['Q'][8743:], "answer":dataset['A'][8743:]})
    #random data
    dataset_random = dataset.sample(frac=1).reset_index(drop=True)  # shufflingí•˜ê³  index reset
    dataset = dataset_random
    
    #ê° í•­ëª©ë³„ data
    train_data_Qusetion = pd.DataFrame({"document" : dataset['Q'][:7000], "label" : dataset['label'][:7000]})
    train_data_Answer = pd.DataFrame({"document" : dataset['A'][:7000], "label" : dataset['label'][:7000]})
    
    valid_data_Qusetion = pd.DataFrame({"document" : dataset['Q'][7000:9000], "label" : dataset['label'][7000:9000]})
    valid_data_Answer = pd.DataFrame({"document" : dataset['A'][7000:9000], "label" : dataset['label'][7000:9000]})
    
    test_data_Qusetion = pd.DataFrame({"document" : dataset['Q'][9000:], "label" : dataset['label'][9000:]})
    test_data_Answer = pd.DataFrame({"document" : dataset['A'][9000:], "label" : dataset['label'][9000:]})
    #ê³µë°±ì œê±°
    train_data_Qusetion['document'].replace('', np.nan,inplace=True)
    valid_data_Qusetion['document'].replace('', np.nan,inplace=True)
    test_data_Qusetion['document'].replace('', np.nan,inplace=True)
    
    #model ë¶ˆëŸ¬ì˜¤ê¸°
    MD_NAME = "bert-base-multilingual-cased"
    tokenizer = AutoTokenizer.from_pretrained(MD_NAME)
    
    #tokenizer ìƒì„±
    tokenizer_train_sentences = tokenizer(
        list(train_data_Qusetion['document']),
        return_tensors="pt",
        padding = True,
        truncation=True,
        add_special_tokens=True
    )
    tokenizer_valid_sentences = tokenizer(
        list(valid_data_Qusetion['document']),
        return_tensors="pt",
        padding = True,
        truncation=True,
        add_special_tokens=True
    )
    tokenizer_test_sentences = tokenizer(
        list(test_data_Qusetion['document']),
        return_tensors="pt",
        padding = True,
        truncation=True,
        add_special_tokens=True
    )
    
    #label ë¶„ë¥˜
    train_label = train_data_Qusetion['label'].values
    test_label = test_data_Qusetion['label'].values
    valid_label = valid_data_Qusetion['label'].values
    
     #dataset ìƒì„±
    train_dataset = SingleSentDataset(tokenizer_train_sentences, train_label)
    valid_dataset = SingleSentDataset(tokenizer_valid_sentences, valid_label)
    test_dataset = SingleSentDataset(tokenizer_test_sentences, test_label)
    
    #training arguments(ì¡°ê±´)ì„¤ì •
    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        save_strategy="epoch",
        evaluation_strategy="epoch",
        num_train_epochs=10,              # total number of training epochs
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=32,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='logs',            # directory for storing logs
        logging_steps=10,
        save_steps=300,
        save_total_limit=1,
        metric_for_best_model="accuracy"
    )
    
    #Bert model label ë³€ê²½
    config = BertConfig.from_pretrained(MD_NAME)
    config.num_labels = 3
    model = BertForSequenceClassification(config) 

    metric = load_metric("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return metric.compute(predictions=predictions, references=labels)


    model.to(device)

    trainer = Trainer(
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        # training dataset
        #vocab_size=500000
        compute_metrics = compute_metrics
    )

    trainer.train()
    
    nlp_sentence_classif = pipeline('sentiment-analysis',model=model, tokenizer=tokenizer, device=0)

    #cls ìœ ì‚¬ë„ êµ¬í•˜ëŠ” ë¶€ë¶„
    MODEL_NAME = "bert-base-multilingual-cased"
    tokenizer_cls = AutoTokenizer.from_pretrained(MODEL_NAME)
    model_cls = AutoModel.from_pretrained(MODEL_NAME)

    #labelë³„ question datalist ìƒì„±
    chatbot_zero_Qlist = []
    for data in dataset_zero['Question']:
        chatbot_zero_Qlist.append(data)

    chatbot_one_Qlist = []
    for data in dataset_one['Question']:
        chatbot_one_Qlist.append(data)

    chatbot_two_Qlist = []
    for data in dataset_two['Question']:
        chatbot_two_Qlist.append(data)
    #labelë³„ answer datalist ìƒì„±   
    chatbot_zero_Alist = []
    for data in dataset_zero['answer']:
        chatbot_zero_Alist.append(data)

    chatbot_one_Alist = []
    for data in dataset_one['answer']:
        chatbot_one_Alist.append(data)

    chatbot_two_Alist = []
    for data in dataset_two['answer']:
        chatbot_two_Alist.append(data)
        
    #cls tokenizer
    def get_cls_token(sent) : 
      model_cls.eval()
      tokenized_sent = tokenizer_cls(
          sent,
          return_tensors = "pt",
          truncation = True,
          add_special_tokens=True,
          max_length = 128
      )
      with torch.no_grad():
        outputs = model_cls(**tokenized_sent)
      logits = outputs.last_hidden_state[:,0,:].detach().cpu().numpy()
      return logits

    #ê° data tokenizing
    data_cls_zero = []
    data_cls_one = []
    data_cls_two = []

    for q in tqdm(chatbot_zero_Qlist) :
      q_cls = get_cls_token(q)
      data_cls_zero.append(q_cls)
    data_cls_zero = np.array(data_cls_zero).squeeze(axis=1)

    for q in tqdm(chatbot_one_Qlist) :
      q_cls = get_cls_token(q)
      data_cls_one.append(q_cls)
    data_cls_one = np.array(data_cls_one).squeeze(axis=1)

    for q in tqdm(chatbot_two_Qlist) :
      q_cls = get_cls_token(q)
      data_cls_two.append(q_cls)
    data_cls_two = np.array(data_cls_two).squeeze(axis=1)
    
    #ì§ˆë¬¸ ì…ë ¥, tokenizing
    query = 'ë¼ë””ë¼íŒ'
    query_cls_hidden = get_cls_token(query)
    
    #ë‹µë³€ ìƒì„±, ì¶œë ¥
    if find_Badword(query) :
        print('ìš•ì„¤ì´ í¬í•¨ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤')

    elif sentences_predict(query, model) == 0:
        cos_sim = cosine_similarity(query_cls_hidden, data_cls_zero)

        top_question = np.argmax(cos_sim)

        print('ë‚˜ì˜ ì§ˆë¬¸: ', query)
        print('ì €ì¥ëœ ë‹µë³€: ', chatbot_zero_Alist[top_question])

    elif sentences_predict(query, model) == 1:
        cos_sim = cosine_similarity(query_cls_hidden, data_cls_one)

        top_question = np.argmax(cos_sim)

        print('ë‚˜ì˜ ì§ˆë¬¸: ', query)
        print('ì €ì¥ëœ ë‹µë³€: ', chatbot_one_Alist[top_question])

    elif sentences_predict(query, model) == 2:
        cos_sim = cosine_similarity(query_cls_hidden, data_cls_two)

        top_question = np.argmax(cos_sim)

        print('ë‚˜ì˜ ì§ˆë¬¸: ', query)
        print('ì €ì¥ëœ ë‹µë³€: ', chatbot_two_Alist[top_question])
    
if __name__ == "__main__":
    main()