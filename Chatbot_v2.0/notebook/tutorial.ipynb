{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "GvPftIiCQrrW"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words'...\n",
      "remote: Enumerating objects: 480, done.\u001b[K\n",
      "remote: Total 480 (delta 0), reused 0 (delta 0), pack-reused 480\u001b[K\n",
      "Receiving objects: 100% (480/480), 110.14 KiB | 5.01 MiB/s, done.\n",
      "Resolving deltas: 100% (249/249), done.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'korean-bad-words'...\n",
      "remote: Enumerating objects: 56, done.\u001b[K\n",
      "remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56\u001b[K\n",
      "Unpacking objects: 100% (56/56), done.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'Gentleman'...\n",
      "remote: Enumerating objects: 55, done.\u001b[K\n",
      "remote: Total 55 (delta 0), reused 0 (delta 0), pack-reused 55\u001b[K\n",
      "Unpacking objects: 100% (55/55), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words\n",
    "!git clone https://github.com/doublems/korean-bad-words\n",
    "!git clone https://github.com/organization/Gentleman/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비속어 사전 01\n",
    "slang_list_01 = []\n",
    "f = open(\"./List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/ko\", 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    slang_list_01.append(line)\n",
    "f.close()\n",
    "\n",
    "# 비속어 사전 02\n",
    "slang_list_02 = []\n",
    "f = open(\"./korean-bad-words/korean-bad-words.md\", 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    slang_list_02.append(line)\n",
    "f.close()\n",
    "\n",
    "# 비속어 사전 03\n",
    "with open('./Gentleman/resources/badwords.json') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "slang_list_03 = json_data[\"badwords\"]\n",
    "\n",
    "# 비속어 사전 통합\n",
    "slang_list = slang_list_01 + slang_list_02 + slang_list_03\n",
    "slang_list = list(set(slang_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Badword(sent) :\n",
    "    Badin = False\n",
    "    for word in slang_list :\n",
    "        if word in sent :\n",
    "            Badin = True\n",
    "    return Badin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TQWjyM--QuhE"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h6LR1VxPQy-y"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSqI4KwKkVOt",
    "outputId": "3fcacb2d-d607-4c02-96bc-21b4c84c95b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s2YHsZ3IkVY2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop_duplicates(subset=['Q'], inplace=True)\n",
    "dataset['label'][8860]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2919"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_random = dataset.sample(frac=1).reset_index(drop=True)  # shuffling하고 index reset\n",
    "# dataset_random\n",
    "dataset_zero=pd.DataFrame({\"Question\":dataset['Q'][:5261], \"answer\":dataset['A'][:5261]})\n",
    "dataset_one=pd.DataFrame({\"Question\":dataset['Q'][5261:8743], \"answer\":dataset['A'][5261:8743]})\n",
    "dataset_two=pd.DataFrame({\"Question\":dataset['Q'][8743:], \"answer\":dataset['A'][8743:]})\n",
    "\n",
    "len(dataset_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_random = dataset.sample(frac=1).reset_index(drop=True)  # shuffling하고 index reset\n",
    "dataset_random\n",
    "dataset = dataset_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사랑하는 사람 잊는 법</td>\n",
       "      <td>충분히 아파하면서 충분한 시간이 필요할 거예요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>술 안 먹으면 친구랑 뭐하지</td>\n",
       "      <td>아무것도 안해도 재밌을 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>짝남이 고시생이면 기다려야 하나요?</td>\n",
       "      <td>기다릴 수 있다면 기다리세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이젠 짝사랑하는게 무섭고 두려워.</td>\n",
       "      <td>이제 짝사랑 말고 둘이서 하는 사랑하세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>숨막히네</td>\n",
       "      <td>숨 쉴 수 있을 때가 올 거예요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11657</th>\n",
       "      <td>짝남한테 같이 공부하러 가자고 해볼까?</td>\n",
       "      <td>공부 도와달라고 해보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11658</th>\n",
       "      <td>회사 일이 너무 많아</td>\n",
       "      <td>너무 무리하지 마세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11659</th>\n",
       "      <td>9년이 넘는 연애를 끝냅니다</td>\n",
       "      <td>공허함이 더 크시겠어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11660</th>\n",
       "      <td>내가 좋아하는 가수 컴백한다</td>\n",
       "      <td>기다렸나봐요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11661</th>\n",
       "      <td>이게 잠수이별인가???</td>\n",
       "      <td>상대방이 예의가 없네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11662 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Q                           A  label\n",
       "0               사랑하는 사람 잊는 법  충분히 아파하면서 충분한 시간이 필요할 거예요.      2\n",
       "1            술 안 먹으면 친구랑 뭐하지           아무것도 안해도 재밌을 거예요.      0\n",
       "2        짝남이 고시생이면 기다려야 하나요?            기다릴 수 있다면 기다리세요.      2\n",
       "3         이젠 짝사랑하는게 무섭고 두려워.     이제 짝사랑 말고 둘이서 하는 사랑하세요.      2\n",
       "4                       숨막히네          숨 쉴 수 있을 때가 올 거예요.      1\n",
       "...                      ...                         ...    ...\n",
       "11657  짝남한테 같이 공부하러 가자고 해볼까?              공부 도와달라고 해보세요.      2\n",
       "11658            회사 일이 너무 많아                너무 무리하지 마세요.      0\n",
       "11659        9년이 넘는 연애를 끝냅니다               공허함이 더 크시겠어요.      1\n",
       "11660        내가 좋아하는 가수 컴백한다                     기다렸나봐요.      0\n",
       "11661           이게 잠수이별인가???               상대방이 예의가 없네요.      1\n",
       "\n",
       "[11662 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_Qusetion = pd.DataFrame({\"document\" : dataset['Q'][:7000], \"label\" : dataset['label'][:7000]})\n",
    "train_data_Answer = pd.DataFrame({\"document\" : dataset['A'][:7000], \"label\" : dataset['label'][:7000]})\n",
    "valid_data_Qusetion = pd.DataFrame({\"document\" : dataset['Q'][7000:9000], \"label\" : dataset['label'][7000:9000]})\n",
    "valid_data_Answer = pd.DataFrame({\"document\" : dataset['A'][7000:9000], \"label\" : dataset['label'][7000:9000]})\n",
    "test_data_Qusetion = pd.DataFrame({\"document\" : dataset['Q'][9000:], \"label\" : dataset['label'][9000:]})\n",
    "test_data_Answer = pd.DataFrame({\"document\" : dataset['A'][9000:], \"label\" : dataset['label'][9000:]})\n",
    "train_data_Qusetion['document'].replace('', np.nan,inplace=True)\n",
    "valid_data_Qusetion['document'].replace('', np.nan,inplace=True)\n",
    "test_data_Qusetion['document'].replace('', np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3184\n",
       "1    2084\n",
       "2    1732\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_Qusetion['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    905\n",
       "1    581\n",
       "2    514\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_Qusetion['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1172\n",
       "1     817\n",
       "2     673\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_Answer['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gs1L63CORCBi"
   },
   "outputs": [],
   "source": [
    "train_data_Qusetion = pd.DataFrame({\"document\" : dataset['Q'][:7000], \"label\" : dataset['label'][:7000]})\n",
    "train_data_Answer = pd.DataFrame({\"document\" : dataset['A'][:7000], \"label\" : dataset['label'][:7000]})\n",
    "valid_data_Qusetion = pd.DataFrame({\"document\" : dataset['Q'][7000:9000], \"label\" : dataset['label'][7000:9000]})\n",
    "valid_data_Answer = pd.DataFrame({\"document\" : dataset['A'][7000:9000], \"label\" : dataset['label'][7000:9000]})\n",
    "test_data_Qusetion = pd.DataFrame({\"document\" : dataset['Q'][9000:], \"label\" : dataset['label'][9000:]})\n",
    "test_data_Answer = pd.DataFrame({\"document\" : dataset['A'][9000:], \"label\" : dataset['label'][9000:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dHW_BMHqRMiS"
   },
   "outputs": [],
   "source": [
    "train_data_Qusetion['document'].replace('', np.nan,inplace=True)\n",
    "valid_data_Qusetion['document'].replace('', np.nan,inplace=True)\n",
    "test_data_Qusetion['document'].replace('', np.nan,inplace=True)\n",
    "# train_data_Answer['document'].replace('', np.nan,inplace=True)\n",
    "# test_data_Answer['document'].replace('', np.nan,inplace=True)\n",
    "# train_data_Qusetion = train_data_Qusetion.dropna(how = 'any')\n",
    "# test_data_Qusetion = test_data_Qusetion.dropna(how = 'any')\n",
    "# train_data_Qusetion = train_data_Answer.dropna(how = 'any')\n",
    "# test_data_Answer = test_data_Answer.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "2662\n",
      "7000\n",
      "2662\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_Answer))\n",
    "print(len(test_data_Qusetion))\n",
    "print(len(train_data_Answer))\n",
    "print(len(test_data_Answer))\n",
    "print(len(valid_data_Answer))\n",
    "print(len(valid_data_Qusetion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27CC9mT3SDeV",
    "outputId": "ce9cf136-8347-4206-82c0-a5f331a5860f"
   },
   "outputs": [],
   "source": [
    "MD_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MD_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UAvGvOYFSGhv"
   },
   "outputs": [],
   "source": [
    "tokenizer_train_sentences = tokenizer(\n",
    "    list(train_data_Qusetion['document']),\n",
    "    return_tensors=\"pt\",\n",
    "    padding = True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_valid_sentences = tokenizer(\n",
    "    list(valid_data_Qusetion['document']),\n",
    "    return_tensors=\"pt\",\n",
    "    padding = True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rSXJqogbSO5n"
   },
   "outputs": [],
   "source": [
    "tokenizer_test_sentences = tokenizer(\n",
    "    list(test_data_Qusetion['document']),\n",
    "    return_tensors=\"pt\",\n",
    "    padding = True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7r6hRALJSSqo"
   },
   "outputs": [],
   "source": [
    "train_label = train_data_Qusetion['label'].values\n",
    "test_label = test_data_Qusetion['label'].values\n",
    "valid_label = valid_data_Qusetion['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(valid_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKyq-qzOksJ7",
    "outputId": "d2974e13-6548-418e-907d-43434a81a29f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBG-oYFCTJ78",
    "outputId": "aa8fdedf-2bd6-4039-c462-ed309d121306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=36, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['[CLS]', '사', '##랑', '##하는', '사', '##람', '잊', '##는', '법', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[101, 9405, 62200, 12178, 9405, 61250, 9649, 11018, 9341, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_train_sentences[0])\n",
    "print(tokenizer_train_sentences[0].tokens)\n",
    "print(tokenizer_train_sentences[0].ids)\n",
    "print(tokenizer_train_sentences[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=34, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['[CLS]', '나', '##한', '##테', '##만', '##은', '완', '##전', '솔', '##직', '##했', '##으면', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[101, 8982, 11102, 119351, 19105, 10892, 9591, 16617, 9451, 33077, 119424, 80046, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_test_sentences[0])\n",
    "print(tokenizer_test_sentences[0].tokens)\n",
    "print(tokenizer_test_sentences[0].ids)\n",
    "print(tokenizer_test_sentences[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "A_PF9Co8W1hN"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-jGsVCFrXCk4"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1MKPRbOvSdTu"
   },
   "outputs": [],
   "source": [
    "class SingleSentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,   9405,  62200,  ...,      0,      0,      0],\n",
       "        [   101,   9463,   9521,  ...,      0,      0,      0],\n",
       "        [   101,   9716,  37004,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [   101,   8987, 119196,  ...,      0,      0,      0],\n",
       "        [   101,   9568, 119121,  ...,      0,      0,      0],\n",
       "        [   101,   9781,  17196,  ...,      0,      0,      0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_train_sentences['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "du9ri-miZvqZ"
   },
   "outputs": [],
   "source": [
    "#dataset 생성\n",
    "train_dataset = SingleSentDataset(tokenizer_train_sentences, train_label)\n",
    "valid_dataset = SingleSentDataset(tokenizer_valid_sentences, valid_label)\n",
    "test_dataset = SingleSentDataset(tokenizer_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GY3WtvcrXB5V"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyWvba28Sgqr",
    "outputId": "6a56e1bf-0c7f-4b51-bdf0-6768aa3a6651"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_steps=300,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(MD_NAME)\n",
    "config.num_labels = 3\n",
    "model = BertForSequenceClassification(config) \n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XaaE49glYprj",
    "outputId": "f028aeb6-b681-42e3-8935-2e859bb4e548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(MD_NAME)\n",
    "# model.classifier.out_features = 3\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    # training dataset\n",
    "    #vocab_size=500000\n",
    "    compute_metrics = compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "NqDsxOojZRo1",
    "outputId": "e59243c4-34b0-4eb9-fa3f-a522892c492d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4380\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mddrongy\u001b[0m (\u001b[33mwinner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/ml/wandb/run-20220528_103201-2k7wu35t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/winner/huggingface/runs/2k7wu35t\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/winner/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4380' max='4380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4380/4380 16:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.586598</td>\n",
       "      <td>0.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.618328</td>\n",
       "      <td>0.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>0.572009</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.507587</td>\n",
       "      <td>0.818500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.561121</td>\n",
       "      <td>0.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>0.610135</td>\n",
       "      <td>0.813500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.709696</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.814616</td>\n",
       "      <td>0.813500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.920227</td>\n",
       "      <td>0.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.939265</td>\n",
       "      <td>0.814500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-438\n",
      "Configuration saved in ./results/checkpoint-438/config.json\n",
      "Model weights saved in ./results/checkpoint-438/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3942] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-876\n",
      "Configuration saved in ./results/checkpoint-876/config.json\n",
      "Model weights saved in ./results/checkpoint-876/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4380] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1314\n",
      "Configuration saved in ./results/checkpoint-1314/config.json\n",
      "Model weights saved in ./results/checkpoint-1314/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-438] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1752\n",
      "Configuration saved in ./results/checkpoint-1752/config.json\n",
      "Model weights saved in ./results/checkpoint-1752/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-876] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2190\n",
      "Configuration saved in ./results/checkpoint-2190/config.json\n",
      "Model weights saved in ./results/checkpoint-2190/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1314] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2628\n",
      "Configuration saved in ./results/checkpoint-2628/config.json\n",
      "Model weights saved in ./results/checkpoint-2628/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2190] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-3066\n",
      "Configuration saved in ./results/checkpoint-3066/config.json\n",
      "Model weights saved in ./results/checkpoint-3066/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2628] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-3504\n",
      "Configuration saved in ./results/checkpoint-3504/config.json\n",
      "Model weights saved in ./results/checkpoint-3504/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3066] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-3942\n",
      "Configuration saved in ./results/checkpoint-3942/config.json\n",
      "Model weights saved in ./results/checkpoint-3942/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3504] due to args.save_total_limit\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-4380\n",
      "Configuration saved in ./results/checkpoint-4380/config.json\n",
      "Model weights saved in ./results/checkpoint-4380/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3942] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4380, training_loss=0.37397694903081413, metrics={'train_runtime': 998.4711, 'train_samples_per_second': 70.107, 'train_steps_per_second': 4.387, 'total_flos': 1295011352880000.0, 'train_loss': 0.37397694903081413, 'epoch': 10.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "kpS0A2r_k7If"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "2qgBKq3TbQ9X"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2662\n",
      "  Batch size = 32\n",
      "/tmp/ipykernel_11898/3231692829.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9356488585472107,\n",
       " 'eval_accuracy': 0.8110443275732532,\n",
       " 'eval_f1': 0.80923975594823,\n",
       " 'eval_precision': 0.8101369347176312,\n",
       " 'eval_recall': 0.8084504381873856,\n",
       " 'eval_runtime': 6.4986,\n",
       " 'eval_samples_per_second': 409.625,\n",
       " 'eval_steps_per_second': 12.926}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict함수\n",
    "def sentences_predict(sent):\n",
    "    model.eval()\n",
    "    tokenized_sent = tokenizer(\n",
    "            sent,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128\n",
    "    )\n",
    "    tokenized_sent.to(device)\n",
    "    \n",
    "    with torch.no_grad():# 그라디엔트 계산 비활성화\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_sent['input_ids'],\n",
    "            attention_mask=tokenized_sent['attention_mask'],\n",
    "            token_type_ids=tokenized_sent['token_type_ids']\n",
    "            )\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sentences_predict(\"영화 개재밌어 ㅋㅋㅋㅋㅋ\"))\n",
    "print(sentences_predict(\"진짜 재미없네요 ㅋㅋ\"))\n",
    "print(sentences_predict(\"너 때문에 진짜 짜증나\"))\n",
    "print(sentences_predict(\"정말 재밌고 좋았어요.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_2', 'score': 0.9979351758956909}]\n",
      "[{'label': 'LABEL_1', 'score': 0.9485467672348022}]\n",
      "[{'label': 'LABEL_1', 'score': 0.9443480968475342}]\n",
      "[{'label': 'LABEL_2', 'score': 0.998594343662262}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_sentence_classif = pipeline('sentiment-analysis',model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "##test\n",
    "print(nlp_sentence_classif('영화 개재밌어 ㅋㅋㅋㅋㅋ'))\n",
    "print(nlp_sentence_classif('진짜 재미없네요 ㅋㅋ'))\n",
    "print(nlp_sentence_classif('너 때문에 진짜 짜증나'))\n",
    "print(nlp_sentence_classif('정말 재밌고 좋았어요.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9967002272605896}]\n"
     ]
    }
   ],
   "source": [
    "print(nlp_sentence_classif('김규리 바보'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /opt/ml/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /opt/ml/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /opt/ml/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#cls 유사도 구하는 부분\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer_cls = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_cls = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_zero_Qlist = []\n",
    "for data in dataset_zero['Question']:\n",
    "    chatbot_zero_Qlist.append(data)\n",
    "    \n",
    "chatbot_one_Qlist = []\n",
    "for data in dataset_one['Question']:\n",
    "    chatbot_one_Qlist.append(data)\n",
    "    \n",
    "chatbot_two_Qlist = []\n",
    "for data in dataset_two['Question']:\n",
    "    chatbot_two_Qlist.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_zero_Alist = []\n",
    "for data in dataset_zero['answer']:\n",
    "    chatbot_zero_Alist.append(data)\n",
    "\n",
    "chatbot_one_Alist = []\n",
    "for data in dataset_one['answer']:\n",
    "    chatbot_one_Alist.append(data)\n",
    "    \n",
    "chatbot_two_Alist = []\n",
    "for data in dataset_two['answer']:\n",
    "    chatbot_two_Alist.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_token(sent) : \n",
    "  model_cls.eval()\n",
    "  tokenized_sent = tokenizer_cls(\n",
    "      sent,\n",
    "      return_tensors = \"pt\",\n",
    "      truncation = True,\n",
    "      add_special_tokens=True,\n",
    "      max_length = 128\n",
    "  )\n",
    "  with torch.no_grad():\n",
    "    outputs = model_cls(**tokenized_sent)\n",
    "  logits = outputs.last_hidden_state[:,0,:].detach().cpu().numpy()\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5261/5261 [15:39<00:00,  5.60it/s]  \n",
      "100%|██████████| 3482/3482 [09:45<00:00,  5.95it/s] \n",
      "100%|██████████| 2919/2919 [08:57<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "data_cls_zero = []\n",
    "data_cls_one = []\n",
    "data_cls_two = []\n",
    "\n",
    "for q in tqdm(chatbot_zero_Qlist) :\n",
    "  q_cls = get_cls_token(q)\n",
    "  data_cls_zero.append(q_cls)\n",
    "data_cls_zero = np.array(data_cls_zero).squeeze(axis=1)\n",
    "\n",
    "for q in tqdm(chatbot_one_Qlist) :\n",
    "  q_cls = get_cls_token(q)\n",
    "  data_cls_one.append(q_cls)\n",
    "data_cls_one = np.array(data_cls_one).squeeze(axis=1)\n",
    "\n",
    "for q in tqdm(chatbot_two_Qlist) :\n",
    "  q_cls = get_cls_token(q)\n",
    "  data_cls_two.append(q_cls)\n",
    "data_cls_two = np.array(data_cls_two).squeeze(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "query = '라디라펌'\n",
    "query_cls_hidden = get_cls_token(query)\n",
    "print(sentences_predict(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나의 질문:  라디라펌\n",
      "저장된 답변:  있으면 좋죠.\n"
     ]
    }
   ],
   "source": [
    "if find_Badword(query) :\n",
    "    print('욕설이 포함된 질문입니다')\n",
    "\n",
    "elif sentences_predict(query) == 0:\n",
    "    cos_sim = cosine_similarity(query_cls_hidden, data_cls_zero)\n",
    "\n",
    "    top_question = np.argmax(cos_sim)\n",
    "\n",
    "    print('나의 질문: ', query)\n",
    "    print('저장된 답변: ', chatbot_zero_Alist[top_question])\n",
    "\n",
    "elif sentences_predict(query) == 1:\n",
    "    cos_sim = cosine_similarity(query_cls_hidden, data_cls_one)\n",
    "\n",
    "    top_question = np.argmax(cos_sim)\n",
    "\n",
    "    print('나의 질문: ', query)\n",
    "    print('저장된 답변: ', chatbot_one_Alist[top_question])\n",
    "    \n",
    "elif sentences_predict(query) == 2:\n",
    "    cos_sim = cosine_similarity(query_cls_hidden, data_cls_two)\n",
    "\n",
    "    top_question = np.argmax(cos_sim)\n",
    "    \n",
    "    print('나의 질문: ', query)\n",
    "    print('저장된 답변: ', chatbot_two_Alist[top_question])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tes",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
